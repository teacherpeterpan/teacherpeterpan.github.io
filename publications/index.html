<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Liangming Pan </title> <meta name="author" content="Liangming Pan"> <meta name="description" content="* denotes equal contribution"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/avatar2.jpg?979379eb3bb27557e1ff6d4589365033"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://teacherpeterpan.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Liangming Pan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">Home </a> </li> <li class="nav-item "> <a class="nav-link" href="/bio/">Bio </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks_teaching/">Talks &amp; Teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/service/">Service </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications / Preprints</h1> <p class="post-description">* denotes equal contribution</p> </header> <article> <p>Please see my <a href="https://scholar.google.com/citations?user=JcjjOTUAAAAJ" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i> Google Scholar</a> or <a href="https://semanticscholar.org/author/Liangming-Pan/3470231" target="_blank" rel="noopener noreferrer"><i class="ai ai-semantic-scholar"></i> Semantic Scholar</a> for the most up-to-date list of publications. </p> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">Preprints</h2> <h3 class="bibliography">2024</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="ma2024mmlongbenchdocbenchmarkinglongcontextdocument" class="col-sm-10"> <div class="title">MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations</div> <div class="author" style="font-size: 90%;"> Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, Pan Zhang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Yu-Gang Jiang, Jiaqi Wang, Yixin Cao, and Aixin Sun </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2407.01523" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/mayubo2333/MMLongBench-Doc" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://huggingface.co/datasets/yubo2333/MMLongBench-Doc" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://mayubo2333.github.io/MMLongBench-Doc/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLongBench-Doc, a long-context, multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct from previous datasets, it is constructed upon 130 lengthy PDF-formatted documents with an average of 49.4 pages and 20,971 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e. page number). Moreover, 33.2% of the questions are cross-page questions requiring evidence across multiple pages. 22.8% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores 31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">ma2024mmlongbenchdocbenchmarkinglongcontextdocument</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Yubo and Zang, Yuhang and Chen, Liangyu and Chen, Meiqi and Jiao, Yizhu and Li, Xinze and Lu, Xinyuan and Liu, Ziyu and Ma, Yan and Dong, Xiaoyi and Zhang, Pan and Pan, Liangming and Jiang, Yu-Gang and Wang, Jiaqi and Cao, Yixin and Sun, Aixin}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2407.01523}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2407.01523}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="yao2024seakrselfawareknowledgeretrieval" class="col-sm-10"> <div class="title">SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation</div> <div class="author" style="font-size: 90%;"> Zijun Yao, Weijian Qi,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Shulin Cao, Linmei Hu, Weichuan Liu, Lei Hou, and Juanzi Li </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2406.19215" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/thu-keg/seakr" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM’s self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">yao2024seakrselfawareknowledgeretrieval</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yao, Zijun and Qi, Weijian and Pan, Liangming and Cao, Shulin and Hu, Linmei and Liu, Weichuan and Hou, Lei and Li, Juanzi}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2406.19215}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2406.19215}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="wong2024distilrrtransferringcoderepair" class="col-sm-10"> <div class="title">DistiLRR: Transferring Code Repair for Low-Resource Programming Languages</div> <div class="author" style="font-size: 90%;"> Kyle Wong, Alfonso Amayuelas,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2406.14867" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/kylewong288/distilrr" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Large language models (LLMs) have shown remarkable performance on code generation tasks. A recent application of LLMs for code generation is iterative code repair, where a model fixes an incorrect program by rationalizing about errors and generating a new program. However, code repair is primarily studied on high-resource languages like Python, and the framework’s efficacy is under-explored on low-resource languages. To apply code repair for low-resource languages, we propose Distilling Low-Resource Repairs (DistiLRR), an approach that transfers the reasoning and code generation ability from a teacher model to a student model. Our results show that DistiLRR consistently outperforms baselines on low-resource languages, but has similar performance on high-resource languages. To investigate this behavior, we perform a further analysis and find that the correlation between rationale quality and code correctness is weaker than previously perceived. We hypothesize this weakness is magnified in low-resource settings where base models lack deep knowledge of a programming language, leading to wavering benefits of code repair between high-resource and low-resource languages.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">wong2024distilrrtransferringcoderepair</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{DistiLRR: Transferring Code Repair for Low-Resource Programming Languages}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wong, Kyle and Amayuelas, Alfonso and Pan, Liangming and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2406.14867}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2406.14867}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="amayuelas2024multiagentcollaborationattackinvestigating" class="col-sm-10"> <div class="title">MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate</div> <div class="author" style="font-size: 90%;"> Alfonso Amayuelas, Xianjun Yang, Antonis Antoniades, Wenyue Hua,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, and William Wang </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2406.14711" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary’s effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model’s persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">amayuelas2024multiagentcollaborationattackinvestigating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Amayuelas, Alfonso and Yang, Xianjun and Antoniades, Antonis and Hua, Wenyue and Pan, Liangming and Wang, William}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2406.14711}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2406.14711}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="wu2024updatinglanguagemodelsunstructured" class="col-sm-10"> <div class="title">Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing</div> <div class="author" style="font-size: 90%;"> Xiaobao Wu,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, William Yang Wang, and Anh Tuan Luu </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.18909" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further show that this challenge persists even if we extract triplets as structured facts. Our analysis discloses key insights to motivate future research in UKE for more practical knowledge editing.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">wu2024updatinglanguagemodelsunstructured</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Xiaobao and Pan, Liangming and Wang, William Yang and Luu, Anh Tuan}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2402.18909}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2402.18909}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="ma2024sciagenttoolaugmentedlanguagemodels" class="col-sm-10"> <div class="title">SciAgent: Tool-augmented Language Models for Scientific Reasoning</div> <div class="author" style="font-size: 90%;"> Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Yujiu Yang, Yixin Cao, Aixin Sun, Hany Awadalla, and Weizhu Chen </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.11451" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs’ abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">ma2024sciagenttoolaugmentedlanguagemodels</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SciAgent: Tool-augmented Language Models for Scientific Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Yubo and Gou, Zhibin and Hao, Junheng and Xu, Ruochen and Wang, Shuohang and Pan, Liangming and Yang, Yujiu and Cao, Yixin and Sun, Aixin and Awadalla, Hany and Chen, Weizhu}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2402.11451}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2402.11451}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="wang2024factcheckbenchfinegrainedevaluationbenchmark" class="col-sm-10"> <div class="title">Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers</div> <div class="author" style="font-size: 90%;"> Yuxia Wang, Revanth Gangi Reddy, Zain Muhammad Mujahid, Arnav Arora, Aleksandr Rubashevskii, Jiahui Geng, Osama Mohammed Afzal,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Nadav Borenstein, Aditya Pillai, Isabelle Augenstein, Iryna Gurevych, and Preslav Nakov </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2311.09000" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/yuxiaw/factcheck-gpt" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:hMod-77fHWUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-20-4285F4?logo=googlescholar&amp;labelColor=beige" alt="20 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We design and build an annotation tool to speed up the labelling procedure and ease the workload of raters. It allows flexible incorporation of automatic results in any stage, e.g. automatically-retrieved evidence. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims with the best F1=0.53.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">wang2024factcheckbenchfinegrainedevaluationbenchmark</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Yuxia and Reddy, Revanth Gangi and Mujahid, Zain Muhammad and Arora, Arnav and Rubashevskii, Aleksandr and Geng, Jiahui and Afzal, Osama Mohammed and Pan, Liangming and Borenstein, Nadav and Pillai, Aditya and Augenstein, Isabelle and Gurevych, Iryna and Nakov, Preslav}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2311.09000}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2311.09000}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">2023</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">arXiv</a> </abbr> </div> <div id="yang2023surveydetectionllmsgeneratedcontent" class="col-sm-10"> <div class="title">A Survey on Detection of LLMs-Generated Content</div> <div class="author" style="font-size: 90%;"> Xianjun Yang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Xuandong Zhao, Haifeng Chen, Linda Petzold, William Yang Wang, and Wei Cheng </div> <div class="periodical" style="font-size: 90%;"> <em>arXiv preprint</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2310.15654" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xianjun-yang/awesome_papers_on_llms_detection" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:JV2RwH3_ST0C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-23-4285F4?logo=googlescholar&amp;labelColor=beige" alt="23 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@misc</span><span class="p">{</span><span class="nl">yang2023surveydetectionllmsgeneratedcontent</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Survey on Detection of LLMs-Generated Content}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Xianjun and Pan, Liangming and Zhao, Xuandong and Chen, Haifeng and Petzold, Linda and Wang, William Yang and Cheng, Wei}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2310.15654}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CL}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2310.15654}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">Conference &amp; Journal Papers</h2> <h3 class="bibliography">2024</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">TACL</a> </abbr> </div> <div id="pan-etal-2024-automatically" class="col-sm-10"> <div class="title">Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>Transactions of the Association for Computational Linguistics (TACL)</em>, 2024 </div> <div class="periodical"> </div> <div class="periodical font-italic" style="color:red;"> <i class="fa fa-solid fa-star"></i> Oral Presentation at ACL 2024</div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00660/120911" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/poster/tacl2024_poster.pdf" class="badge grey waves-effect mr-1" role="button">Poster</a> <a href="https://github.com/teacherpeterpan/self-correction-llm-papers" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:vV6vV6tmYwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-110-4285F4?logo=googlescholar&amp;labelColor=beige" alt="110 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback—either produced by the LLM itself (self-correction) or some external system—are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pan-etal-2024-automatically</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Saxon, Michael and Xu, Wenda and Nathani, Deepak and Wang, Xinyi and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions of the Association for Computational Linguistics (TACL)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{12}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Cambridge, MA}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{MIT Press}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2024.tacl-1.27}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{484--506}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="xu-etal-2024-faithful" class="col-sm-10"> <div class="title">Faithful Logical Reasoning via Symbolic Chain-of-Thought</div> <div class="author" style="font-size: 90%;"> Jundong Xu, Hao Fei,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Qian Liu, Mong-Li Lee, and Wynne Hsu </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2405.18357" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/aiden0526/symbcot" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-faithful</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Faithful Logical Reasoning via Symbolic Chain-of-Thought}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Jundong and Fei, Hao and Pan, Liangming and Liu, Qian and Lee, Mong{-}Li and Hsu, Wynne}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Thailand}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2405.18357}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="xu-etal-2024-pride" class="col-sm-10"> <div class="title">Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</div> <div class="author" style="font-size: 90%;"> Wenda Xu, Guanglei Zhu, Xuandong Zhao,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Lei Li, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.11436" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/xu1998hz/llm_self_bias" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:3s1wT3WcHBgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-7-4285F4?logo=googlescholar&amp;labelColor=beige" alt="7 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM’s bias in evaluating their own output. In this paper, we formally define LLM’s self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2024-pride</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Wenda and Zhu, Guanglei and Zhao, Xuandong and Pan, Liangming and Li, Lei and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Thailand}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2402.11436}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="alfonso-etal-2024-knowledge" class="col-sm-10"> <div class="title">Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models</div> <div class="author" style="font-size: 90%;"> Alfonso Amayuelas, Kyle Wong,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Wenhu Chen, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Findings of Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.13712" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/amayuelas/knowledge-of-knowledge" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:TQgYirikUcIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-26-4285F4?logo=googlescholar&amp;labelColor=beige" alt="26 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models’ improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">alfonso-etal-2024-knowledge</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Amayuelas, Alfonso and Wong, Kyle and Pan, Liangming and Chen, Wenhu and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Thailand}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2305.13712}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="zhang-etal-2024-knowledge" class="col-sm-10"> <div class="title">The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models</div> <div class="author" style="font-size: 90%;"> Shuo Zhang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Junzhou Zhao, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Findings of Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.13669" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/shuozhangxjtu/mixalign" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:M05iB0D1s5AC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-26-4285F4?logo=googlescholar&amp;labelColor=beige" alt="26 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the effectiveness of MixAlign in improving knowledge alignment by producing high-quality, user-centered clarifications.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-etal-2024-knowledge</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Shuo and Pan, Liangming and Zhao, Junzhou and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Thailand}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2305.13669}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="li-etal-2024-towards" class="col-sm-10"> <div class="title">Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution</div> <div class="author" style="font-size: 90%;"> Xinze Li, Yixin Cao,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Yubo Ma, and Aixin Sun </div> <div class="periodical" style="font-size: 90%;"> <em>In Findings of Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2310.05634" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. Although language attribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new “Conscious Incompetence” setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs’ citation generation, emphasizing the importance of incorporating the “Conscious Incompetence” setting, and the critical role of retrieval accuracy.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li-etal-2024-towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Xinze and Cao, Yixin and Pan, Liangming and Ma, Yubo and Sun, Aixin}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Thailand}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2310.05634}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="wu-etal-2024-modeling" class="col-sm-10"> <div class="title">Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion</div> <div class="author" style="font-size: 90%;"> Xiaobao Wu, Xinshuai Dong,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Thong Nguyen, and Anh Tuan Luu </div> <div class="periodical" style="font-size: 90%;"> <em>In Findings of Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2405.17957" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bobxwu/CFDTM" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Dynamic topic models track the evolution of topics in sequential documents, which have derived various applications like trend analysis and opinion mining. However, existing models suffer from repetitive topic and unassociated topic issues, failing to reveal the evolution and hindering further applications. To address these issues, we break the tradition of simply chaining topics in existing work and propose a novel neural \modelfullname. We introduce a new evolution-tracking contrastive learning method that builds the similarity relations among dynamic topics. This not only tracks topic evolution but also maintains topic diversity, mitigating the repetitive topic issue. To avoid unassociated topics, we further present an unassociated word exclusion method that consistently excludes unassociated words from discovered topics. Extensive experiments demonstrate our model significantly outperforms state-of-the-art baselines, tracking topic evolution with high-quality topics, showing better performance on downstream tasks, and remaining robust to the hyperparameter for evolution intensities.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wu-etal-2024-modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Xiaobao and Dong, Xinshuai and Pan, Liangming and Nguyen, Thong and Luu, Anh Tuan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Thailand}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2405.17957}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ICML</a> </abbr> </div> <div id="wang-etal-2024-understanding" class="col-sm-10"> <div class="title">Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation</div> <div class="author" style="font-size: 90%;"> Xinyi Wang, Alfonso Amayuelas, Kexun Zhang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Wenhu Chen, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In International Conference on Machine Learning (ICML)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.03268" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/wangxinyilinda/lm_random_walk" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and CoT datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang-etal-2024-understanding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Xinyi and Amayuelas, Alfonso and Zhang, Kexun and Pan, Liangming and Chen, Wenhu and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Austria}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2402.03268}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ICML</a> </abbr> </div> <div id="weissburg-etal-2024-tweets" class="col-sm-10"> <div class="title">Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility</div> <div class="author" style="font-size: 90%;"> Iain Xie Weissburg, Mehir Arora, Xinyi Wang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In International Conference on Machine Learning (ICML)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2401.13782" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside controls precisely matched by 9 key covariates. Our statistical and causal inference analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. Given these findings, we advocate for a responsible approach to curation, encouraging influencers to uphold the journalistic standard that includes showcasing diverse research topics, authors, and institutions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">weissburg-etal-2024-tweets</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Weissburg, Iain Xie and Arora, Mehir and Wang, Xinyi and Pan, Liangming and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Machine Learning (ICML)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Austria}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2401.13782}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">TMLR</a> </abbr> </div> <div id="DBLP:journals/corr/abs-2402-16827" class="col-sm-10"> <div class="title">A Survey on Data Selection for Language Models</div> <div class="author" style="font-size: 90%;"> Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>Transactions on Machine Learning Research (TMLR)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2402.16827" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/alon-albalak/data-selection-survey" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Website</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:J_g5lzvAfSwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-22-4285F4?logo=googlescholar&amp;labelColor=beige" alt="22 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required. Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies. To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2402-16827</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Albalak, Alon and Elazar, Yanai and Xie, Sang Michael and Longpre, Shayne and Lambert, Nathan and Wang, Xinyi and Muennighoff, Niklas and Hou, Bairu and Pan, Liangming and Jeong, Haewon and Raffel, Colin and Chang, Shiyu and Hashimoto, Tatsunori and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Survey on Data Selection for Language Models}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.48550/arXiv.2402.16827}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">2023</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> </div> <div id="lu-etal-2023-scitab" class="col-sm-10"> <div class="title">SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables</div> <div class="author" style="font-size: 90%;"> Xinyuan Lu<sup>*</sup>,  <a style="color: #0076df; font-weight: bold;">Liangming Pan<sup>*</sup></a>, Qian Liu, Preslav Nakov, and Min-Yen Kan </div> <div class="periodical" style="font-size: 90%;"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.13186" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2023.emnlp-main.483.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/xinyuanlu00/scitab" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:hFOr9nPyWt4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-8-4285F4?logo=googlescholar&amp;labelColor=beige" alt="8 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lu-etal-2023-scitab</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{SCITAB}: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lu, Xinyuan and Pan, Liangming and Liu, Qian and Nakov, Preslav and Kan, Min-Yen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-main.483}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{7787--7813}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> </div> <div id="nathani-etal-2023-maf" class="col-sm-10"> <div class="title">MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models</div> <div class="author" style="font-size: 90%;"> Deepak Nathani, David Wang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, and William Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2310.12426" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2023.emnlp-main.407.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/deepakn97/maf" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:maZDTaKrznsC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through *self-improvement* using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose **Multi-Aspect Feedback**, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see an improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">nathani-etal-2023-maf</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{MAF}: Multi-Aspect Feedback for Improving Reasoning in Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Nathani, Deepak and Wang, David and Pan, Liangming and Wang, William}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-main.407}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6591--6616}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="xu-etal-2023-instructscore" class="col-sm-10"> <div class="title">INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback</div> <div class="author" style="font-size: 90%;"> Wenda Xu, Danqing Wang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li </div> <div class="periodical" style="font-size: 90%;"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.14282" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2023.emnlp-main.365.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/xu1998hz/sescore3" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://huggingface.co/xu1998hz/InstructScore" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Model</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:ldfaerwXgEUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-40-4285F4?logo=googlescholar&amp;labelColor=beige" alt="40 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate INSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xu-etal-2023-instructscore</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{INSTRUCTSCORE}: Towards Explainable Text Generation Evaluation with Automatic Feedback}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xu, Wenda and Wang, Danqing and Pan, Liangming and Song, Zhenqiao and Freitag, Markus and Wang, William and Li, Lei}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-main.365}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5967--5994}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> </div> <div id="diao-etal-2023-doolittle" class="col-sm-10"> <div class="title">Doolittle: Benchmarks and Corpora for Academic Writing Formalization</div> <div class="author" style="font-size: 90%;"> Shizhe Diao, Yongyu Lei,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Tianqing Fang, Wangchunshu Zhou, Sedrick Keh, Min-Yen Kan, and Tong Zhang </div> <div class="periodical" style="font-size: 90%;"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://aclanthology.org/2023.emnlp-main.809" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2023.emnlp-main.809.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/shizhediao/Doolittle" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> </div> <div class="abstract hidden"> <p>Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">diao-etal-2023-doolittle</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Doolittle: Benchmarks and Corpora for Academic Writing Formalization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Diao, Shizhe and Lei, Yongyu and Pan, Liangming and Fang, Tianqing and Zhou, Wangchunshu and Keh, Sedrick and Kan, Min-Yen and Zhang, Tong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-main.809}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13093--13111}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> </div> <div id="pan-etal-2023-logic" class="col-sm-10"> <div class="title">Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Alon Albalak, Xinyi Wang, and William Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.12295" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/teacherpeterpan/logic-llm" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:RHpTSmoSYBkC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-84-4285F4?logo=googlescholar&amp;labelColor=beige" alt="84 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver’s error messages to revise symbolic formalizations. We demonstrate Logic-LM’s effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2023-logic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Logic-{LM}: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-emnlp.248}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3806--3824}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> </div> <div id="pan-etal-2023-risk" class="col-sm-10"> <div class="title">On the Risk of Misinformation Pollution with Large Language Models</div> <div class="author" style="font-size: 90%;"> Yikang Pan<sup>*</sup>,  <a style="color: #0076df; font-weight: bold;">Liangming Pan<sup>*</sup></a>, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.13661" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/MexicanLemonade/LLM-Misinfo-QA" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:mB3voiENLucC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-85-4285F4?logo=googlescholar&amp;labelColor=beige" alt="85 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87%) in the performance of ODQA systems. Moreover, we uncover disparities in the attributes associated with persuading humans and machines, presenting an obstacle to current human-centric approaches to combat misinformation. To mitigate the harm caused by LLM-generated misinformation, we propose three defense strategies: misinformation detection, vigilant prompting, and reader ensemble. These approaches have demonstrated promising results, albeit with certain associated costs. Lastly, we discuss the practicality of utilizing LLMs as automatic misinformation generators and provide relevant resources and code to facilitate future research in this area.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2023-risk</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{On the Risk of Misinformation Pollution with Large Language Models}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Yikang and Pan, Liangming and Chen, Wenhu and Nakov, Preslav and Kan, Min-Yen and Wang, William}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.findings-emnlp.97}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1389--1403}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> </div> <div id="pan-etal-2023-qacheck" class="col-sm-10"> <div class="title">QACheck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Xinyuan Lu, Min-Yen Kan, and Preslav Nakov </div> <div class="periodical" style="font-size: 90%;"> <em>In Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP Demo)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2310.07609" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2023.emnlp-demo.23.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/xinyuanlu00/qacheck" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Fact-checking real-world claims often requires intricate, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning process. To address this, we propose the Question-guided Multi-hop Fact-Checking (QACheck) system, which guides the model’s reasoning process by asking a series of questions critical for verifying a claim. QACheck has five key modules: a claim verifier, a question generator, a question-answering module, a QA validator, and a reasoner. Users can input a claim into QACheck, which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QACheck also provides the source of evidence supporting each question, fostering a transparent, explainable, and user-friendly fact-checking process.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2023-qacheck</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{QAC}heck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Lu, Xinyuan and Kan, Min-Yen and Nakov, Preslav}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP Demo)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Singapore}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.emnlp-demo.23}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{264--273}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="pan-etal-2023-fact" class="col-sm-10"> <div class="title">Fact-Checking Complex Claims with Program-Guided Reasoning</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang Wang, Min-Yen Kan, and Preslav Nakov </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.12744" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2023.acl-long.386.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/teacherpeterpan/ProgramFC" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/poster/programfc_poster.pdf" class="badge grey waves-effect mr-1" role="button">Poster</a> <a href="/assets/pdf/slides/programfc_slides.pdf" class="badge grey waves-effect mr-1" role="button">Slides</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:4JMBOYKVnBMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-41-4285F4?logo=googlescholar&amp;labelColor=beige" alt="41 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at \urlhttps://github.com/mbzuai-nlp/ProgramFC.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2023-fact</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fact-Checking Complex Claims with Program-Guided Reasoning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Wu, Xiaobao and Lu, Xinyuan and Luu, Anh Tuan and Wang, William Yang and Kan, Min-Yen and Nakov, Preslav}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.386}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6981--7004}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="do-etal-2023-modeling" class="col-sm-10"> <div class="title">Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation</div> <div class="author" style="font-size: 90%;"> Xuan Long Do, Bowei Zou, Shafiq Joty, Tran Tai,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Nancy Chen, and Ai Ti Aw </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2305.03088" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2023.acl-long.603.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/dxlong2000/sg-cqg" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Conversational Question Generation (CQG) is a critical task for machines to assist humans in fulfilling their information needs through conversations. The task is generally cast into two different settings: answer-aware and answer-unaware. While the former facilitates the models by exposing the expected answer, the latter is more realistic and receiving growing attentions recently. What-to-ask and how-to-ask are the two main challenges in the answer-unaware setting. To address the first challenge, existing methods mainly select sequential sentences in context as the rationales. We argue that the conversation generated using such naive heuristics may not be natural enough as in reality, the interlocutors often talk about the relevant contents that are not necessarily sequential in context. Additionally, previous methods decide the type of question to be generated (boolean/span-based) implicitly. Modeling the question type explicitly is crucial as the answer, which hints the models to generate a boolean or span-based question, is unavailable. To this end, we present SG-CQG, a two-stage CQG framework. For the what-to-ask stage, a sentence is selected as the rationale from a semantic graph that we construct, and extract the answer span from it. For the how-to-ask stage, a classifier determines the target answer type of the question via two explicit control signals before generating and filtering. In addition, we propose Conv-Distinct, a novel evaluation metric for CQG, to evaluate the diversity of the generated conversation from a context. Compared with the existing answer-unaware CQG models, the proposed SG-CQG achieves state-of-the-art performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">do-etal-2023-modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling What-to-ask and How-to-ask for Answer-unaware Conversational Question Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Do, Xuan Long and Zou, Bowei and Joty, Shafiq and Tai, Tran and Pan, Liangming and Chen, Nancy and Aw, Ai Ti}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.603}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10785--10803}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">AACL / IJCNLP</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="pan-etal-2023-attacking" class="col-sm-10"> <div class="title">Attacking Open-domain Question Answering by Injecting Misinformation</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Wenhu Chen, Min-Yen Kan, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)</em>, 2023 </div> <div class="periodical"> </div> <div class="periodical font-italic" style="color:red;"> <i class="fa fa-solid fa-star"></i> Area Chair Award (Question Answering Track)</div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2110.07803" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/teacherpeterpan/contraqa" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/slides/contraqa_slides.pdf" class="badge grey waves-effect mr-1" role="button">Slides</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:L8Ckcad2t8MC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-12-4285F4?logo=googlescholar&amp;labelColor=beige" alt="12 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>With a rise in false, inaccurate, and misleading information in propaganda, news, and social media, real-world Question Answering (QA) systems face the challenges of synthesizing and reasoning over misinformation-polluted contexts to derive correct answers. This urgency gives rise to the need to make QA systems robust to misinformation, a topic previously unexplored. We study the risk of misinformation to QA models by investigating the sensitivity of open-domain QA models to corpus pollution with misinformation documents. We curate both human-written and model-generated false documents that we inject into the evidence corpus of QA models and assess the impact on the performance of these systems. Experiments show that QA models are vulnerable to even small amounts of evidence contamination brought by misinformation, with large absolute performance drops on all models. Misinformation attack brings more threat when fake documents are produced at scale by neural models or the attacker targets hacking specific questions of interest. To defend against such a threat, we discuss the necessity of building a misinformation-aware QA system that integrates question-answering and misinformation detection in a joint fashion.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2023-attacking</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Attacking Open-domain Question Answering by Injecting Misinformation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Chen, Wenhu and Kan, Min-Yen and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Nusa Dua, Bali}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.ijcnlp-main.35}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{525--539}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">AACL / IJCNLP</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="pan-etal-2023-investigating" class="col-sm-10"> <div class="title">Investigating Zero- and Few-shot Generalization in Fact Verification</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Yunxiang Zhang, and Min-Yen Kan </div> <div class="periodical" style="font-size: 90%;"> <em>In International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2309.09444" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/teacherpeterpan/fact-checking-generalization" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>In this paper, we explore zero- and few-shot generalization for fact verification (FV), which aims to generalize the FV model trained on well-resourced domains (e.g., Wikipedia) to low-resourced domains that lack human annotations. To this end, we first construct a benchmark dataset collection which contains 11 FV datasets representing 6 domains. We conduct an empirical analysis of generalization across these FV datasets, finding that current models generalize poorly. Our analysis reveals that several factors affect generalization, including dataset size, length of evidence, and the type of claims. Finally, we show that two directions of work improve generalization: 1) incorporating domain knowledge via pretraining on specialized domains, and 2) automatically generating training data via claim generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2023-investigating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Investigating Zero- and Few-shot Generalization in Fact Verification}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Zhang, Yunxiang and Kan, Min-Yen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Nusa Dua, Bali}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.ijcnlp-main.34}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{511--524}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">AACL / IJCNLP</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="meng-etal-2023-followupqg" class="col-sm-10"> <div class="title">FollowupQG: Towards Information-Seeking Follow-up Question Generation</div> <div class="author" style="font-size: 90%;"> Yan Meng,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Yixin Cao, and Min-Yen Kan </div> <div class="periodical" style="font-size: 90%;"> <em>In International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2309.05007" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/vivian-my/followupqg" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> </div> <div class="abstract hidden"> <p>Humans ask follow-up questions driven by curiosity, which reflects a creative human cognitive process. We introduce the task of real-world information-seeking follow-up question generation (FQG), which aims to generate follow-up questions seeking a more in-depth understanding of an initial question and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world (initial question, answer, follow-up question) tuples collected from a Reddit forum providing layman-friendly explanations for open-ended questions. In contrast to existing datasets, questions in FOLLOWUPQG use more diverse pragmatic strategies to seek information, and they also show higher-order cognitive skills (such as applying and relating). We evaluate current question generation models on their efficacy for generating follow-up questions, exploring how to generate specific types of follow-up questions based on step-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging benchmark, as model-generated questions are adequate but far from human-raised questions in terms of informativeness and complexity.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">meng-etal-2023-followupqg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{F}ollowup{QG}: Towards Information-Seeking Follow-up Question Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Meng, Yan and Pan, Liangming and Cao, Yixin and Kan, Min-Yen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Natural Language Processing and Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Nusa Dua, Bali}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.ijcnlp-main.17}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{252--271}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">NeurIPS</a> </abbr> </div> <div id="weissburg-etal-2024-tweett" class="col-sm-10"> <div class="title">Efficient Online Data Mixing For Language Model Pre-Training</div> <div class="author" style="font-size: 90%;"> Alon Albalak,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Colin Raffel, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In NeurIPS Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models (R0-FoMo@NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="periodical font-italic" style="color:red;"> <i class="fa fa-solid fa-star"></i> Spotlight Paper</div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2312.02406" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:GnPB-g6toBAC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-6-4285F4?logo=googlescholar&amp;labelColor=beige" alt="6 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The data used to pretrain large language models has a decisive impact on a model’s downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">weissburg-etal-2024-tweett</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Efficient Online Data Mixing For Language Model Pre-Training}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Albalak, Alon and Pan, Liangming and Raffel, Colin and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NeurIPS Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models (R0-FoMo@NeurIPS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New Orleans, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2312.02406}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">WWW</a> </abbr> </div> <div id="DBLP:conf/www/DiaoKPT0023" class="col-sm-10"> <div class="title">Hashtag-Guided Low-Resource Tweet Classification</div> <div class="author" style="font-size: 90%;"> Shizhe Diao, Sedrick Scott Keh,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Zhiliang Tian, Yan Song, and Tong Zhang </div> <div class="periodical" style="font-size: 90%;"> <em>In International World Wide Web Conference (WWW)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2302.10143" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/shizhediao/hashtation" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Social media classification tasks (e.g., tweet sentiment analysis, tweet stance detection) are challenging because social media posts are typically short, informal, and ambiguous. Thus, training on tweets is challenging and demands large-scale human-annotated labels, which are time-consuming and costly to obtain. In this paper, we find that providing hashtags to social media tweets can help alleviate this issue because hashtags can enrich short and ambiguous tweets in terms of various information, such as topic, sentiment, and stance. This motivates us to propose a novel Hashtag-guided Tweet Classification model (HashTation), which automatically generates meaningful hashtags for the input tweet to provide useful auxiliary signals for tweet classification. To generate high-quality and insightful hashtags, our hashtag generation model retrieves and encodes the post-level and entity-level information across the whole corpus. Experiments show that HashTation achieves significant improvements on seven low-resource tweet classification tasks, in which only a limited amount of training data is provided, showing that automatically enriching tweets with model-generated hashtags could significantly reduce the demand for large-scale human-labeled data. Further analysis demonstrates that HashTation is able to generate high-quality hashtags that are consistent with the tweets and their labels.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/www/DiaoKPT0023</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Diao, Shizhe and Keh, Sedrick Scott and Pan, Liangming and Tian, Zhiliang and Song, Yan and Zhang, Tong}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hashtag-Guided Low-Resource Tweet Classification}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International World Wide Web Conference (WWW)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1415--1426}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{ACM}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3543507.3583194}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">AAAI</a> </abbr> </div> <div id="DBLP:conf/aaai/WuDNLPL23" class="col-sm-10"> <div class="title">InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling</div> <div class="author" style="font-size: 90%;"> Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Chaoqun Liu,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, and Anh Tuan Luu </div> <div class="periodical" style="font-size: 90%;"> <em>In AAAI Conference on Artificial Intelligence (AAAI)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2304.03544" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/bobxwu/infoctm" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/aaai/WuDNLPL23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Xiaobao and Dong, Xinshuai and Nguyen, Thong and Liu, Chaoqun and Pan, Liangming and Luu, Anh Tuan}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{InfoCTM: {A} Mutual Information Maximization Perspective of Cross-Lingual
                      Topic Modeling}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AAAI Conference on Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{13763--13771}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{AAAI} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1609/aaai.v37i11.26612}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">2022</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="cao-etal-2022-kqa" class="col-sm-10"> <div class="title">KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base</div> <div class="author" style="font-size: 90%;"> Shulin Cao, Jiaxin Shi,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2007.03875" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/shijx12/kqapro_baselines" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://huggingface.co/datasets/drt/kqa_pro" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:M3ejUd6NZC8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-110-4285F4?logo=googlescholar&amp;labelColor=beige" alt="110 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro can serve for both KBQA and semantic parsing tasks. Experimental results show that state-of-the-art KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cao-etal-2022-kqa</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{KQA} Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Shulin and Shi, Jiaxin and Pan, Liangming and Nie, Lunyiu and Xiang, Yutong and Hou, Lei and Li, Juanzi and He, Bin and Zhang, Hanwang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Dublin, Ireland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.acl-long.422}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6101--6119}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="zhang-etal-2022-interpreting" class="col-sm-10"> <div class="title">Interpreting the Robustness of Neural NLP Models to Textual Perturbations</div> <div class="author" style="font-size: 90%;"> Yunxiang Zhang,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Samson Tan, and Min-Yen Kan </div> <div class="periodical" style="font-size: 90%;"> <em>In Findings of Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2110.07159" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:4DMP91E08xMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-16-4285F4?logo=googlescholar&amp;labelColor=beige" alt="16 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data. However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We further give a causal justification for the learnability metric. We conduct extensive experiments with four prominent NLP models — TextRNN, BERT, RoBERTa and XLNet — over eight types of textual perturbations on three datasets. We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesis.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang-etal-2022-interpreting</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Interpreting the Robustness of Neural {NLP} Models to Textual Perturbations}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Yunxiang and Pan, Liangming and Tan, Samson and Kan, Min-Yen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Dublin, Ireland}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.findings-acl.315}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3993--4007}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">COLING</a> </abbr> </div> <div id="gong-etal-2022-khanq" class="col-sm-10"> <div class="title">KHANQ: A Dataset for Generating Deep Questions in Education</div> <div class="author" style="font-size: 90%;"> Huanli Gong,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, and Hengchang Hu </div> <div class="periodical" style="font-size: 90%;"> <em>In International Conference on Computational Linguistics (COLING)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://aclanthology.org/2022.coling-1.518/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Designing in-depth educational questions is a time-consuming and cognitively demanding task. Therefore, it is intriguing to study how to build Question Generation (QG) models to automate the question creation process. However, existing QG datasets are not suitable for educational question generation because the questions are not real questions asked by humans during learning and can be solved by simply searching for information. To bridge this gap, we present KHANQ, a challenging dataset for educational question generation, containing 1,034 high-quality learner-generated questions seeking an in-depth understanding of the taught online courses in Khan Academy. Each data sample is carefully paraphrased and annotated as a triple of 1) Context: an independent paragraph on which the question is based; 2) Prompt: a text prompt for the question (e.g., the learner’s background knowledge); 3) Question: a deep question based on Context and coherent with Prompt. By conducting a human evaluation on the aspects of appropriateness, coverage, coherence, and complexity, we show that state-of-the-art QG models which perform well on shallow question generation datasets have difficulty in generating useful educational questions. This makes KHANQ a challenging testbed for educational question generation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gong-etal-2022-khanq</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{KHANQ}: A Dataset for Generating Deep Questions in Education}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gong, Huanli and Pan, Liangming and Hu, Hengchang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computational Linguistics (COLING)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Gyeongju, Republic of Korea}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{International Committee on Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.coling-1.518}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5925--5938}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">COLING</a> </abbr> </div> <div id="do-etal-2022-cohs" class="col-sm-10"> <div class="title">CoHS-CQG: Context and History Selection for Conversational Question Generation</div> <div class="author" style="font-size: 90%;"> Xuan Long Do, Bowei Zou,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Nancy F. Chen, Shafiq Joty, and Ai Ti Aw </div> <div class="periodical" style="font-size: 90%;"> <em>In International Conference on Computational Linguistics (COLING)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2209.06652" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/dxlong2000/cohs-cqg" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Conversational question generation (CQG) serves as a vital task for machines to assist humans, such as interactive reading comprehension, through conversations. Compared to traditional single-turn question generation (SQG), CQG is more challenging in the sense that the generated question is required not only to be meaningful, but also to align with the provided conversation. Previous studies mainly focus on how to model the flow and alignment of the conversation, but do not thoroughly study which parts of the context and history are necessary for the model. We believe that shortening the context and history is crucial as it can help the model to optimise more on the conversational alignment property. To this end, we propose CoHS-CQG, a two-stage CQG framework, which adopts a novel CoHS module to shorten the context and history of the input. In particular, it selects the top-p sentences and history turns by calculating the relevance scores of them. Our model achieves state-of-the-art performances on CoQA in both the answer-aware and answer-unaware settings.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">do-etal-2022-cohs</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{C}o{HS}-{CQG}: Context and History Selection for Conversational Question Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Do, Xuan Long and Zou, Bowei and Pan, Liangming and Chen, Nancy F. and Joty, Shafiq and Aw, Ai Ti}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computational Linguistics (COLING)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Gyeongju, Republic of Korea}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{International Committee on Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.coling-1.48}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{580--591}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">NAACL</a> </abbr> </div> <div id="zou-etal-2022-automatic" class="col-sm-10"> <div class="title">Automatic True/False Question Generation for Educational Purpose</div> <div class="author" style="font-size: 90%;"> Bowei Zou, Pengfei Li,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, and Ai Ti Aw </div> <div class="periodical" style="font-size: 90%;"> <em>In NAACL Workshop on Innovative Use of NLP for Building Educational Applications (BEA@NAACL)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://aclanthology.org/2022.bea-1.10/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2022.bea-1.10.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>In field of teaching, true/false questioning is an important educational method for assessing students’ general understanding of learning materials. Manually creating such questions requires extensive human effort and expert knowledge. Question Generation (QG) technique offers the possibility to automatically generate a large number of questions. However, there is limited work on automatic true/false question generation due to the lack of training data and difficulty finding question-worthy content. In this paper, we propose an unsupervised True/False Question Generation approach (TF-QG) that automatically generates true/false questions from a given passage for reading comprehension test. TF-QG consists of a template-based framework that aims to test the specific knowledge in the passage by leveraging various NLP techniques, and a generative framework to generate more flexible and complicated questions by using a novel masking-and-infilling strategy. Human evaluation shows that our approach can generate high-quality and valuable true/false questions. In addition, simulated testing on the generated questions challenges the state-of-the-art inference models from NLI, QA, and fact verification tasks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zou-etal-2022-automatic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Automatic True/False Question Generation for Educational Purpose}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zou, Bowei and Li, Pengfei and Pan, Liangming and Aw, Ai Ti}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{NAACL Workshop on Innovative Use of NLP for Building Educational Applications (BEA@NAACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Seattle, Washington}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2022.bea-1.10}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{61--70}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ICMR</a> </abbr> </div> <div id="DBLP:conf/mir/WuPCJ22" class="col-sm-10"> <div class="title">Ingredient-enriched Recipe Generation from Cooking Videos</div> <div class="author" style="font-size: 90%;"> Jianlong Wu,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Jingjing Chen, and Yu-Gang Jiang </div> <div class="periodical" style="font-size: 90%;"> <em>In International Conference on Multimedia Retrieval (ICMR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://dl.acm.org/doi/abs/10.1145/3512527.3531388/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Cooking video captioning aims to generate the text instructions that describes the cooking procedures presented in the video. Current approaches tend to use large neural models or use more robust feature extractors to increase the expressive ability of features, ignoring the strong correlation between consecutive cooking steps in the video. However, it is intuitive that previous cooking steps can provide clues for the next cooking step. Specially, consecutive cooking steps tend to share the same ingredients. Therefore, accurate ingredients recognition can help to introduce more fine-grained information in captioning. To improve the performance of video procedural caption in cooking video, this paper proposes a framework that introduces ingredient recognition module which uses the copy mechanism to fuse the predicted ingredient information into the generated sentence. Moreover, we integrate the visual information of the previous step into the generation of the current step, and the visual information of the two steps together assist in the generation process. Extensive experiments verify the effectiveness of our propose framework and it achieves the promising performances on both YouCookII and Cooking-COIN datasets.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/mir/WuPCJ22</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wu, Jianlong and Pan, Liangming and Chen, Jingjing and Jiang, Yu{-}Gang}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Ingredient-enriched Recipe Generation from Cooking Videos}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Multimedia Retrieval (ICMR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{249--257}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{ACM}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3512527.3531388}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">RecSys</a> </abbr> </div> <div id="he-etal-2022-modeling" class="col-sm-10"> <div class="title">Modeling and Leveraging Prerequisite Context in Recommendation</div> <div class="author" style="font-size: 90%;"> Hengchang Hu,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Yiding Ran, and Min-Yen Kan </div> <div class="periodical" style="font-size: 90%;"> <em>In RecSys Workshop on Context-Aware Recommender Systems (CARS@RecSys)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2209.11471" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/HoldenHu/PDRS" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Prerequisites can play a crucial role in users’ decision-making yet recommendation systems have not fully utilized such contextual background knowledge. Traditional recommendation systems (RS) mostly enrich user-item interactions where the context consists of static user profiles and item descriptions, ignoring the contextual logic and constraints that underlie them. For example, an RS may recommend an item on the condition that the user has interacted with another item as its prerequisite. Modeling prerequisite context from conceptual side information can overcome this weakness. We propose Prerequisite Driven Recommendation (PDR), a generic context-aware framework where prerequisite context is explicitly modeled to facilitate recommendation. We first design a Prerequisite Knowledge Linking (PKL) algorithm, to curate datasets facilitating PDR research. Employing it, we build a 75k+ high-quality prerequisite concept dataset which spans three domains. We then contribute PDRS, a neural instantiation of PDR. By jointly optimizing both the prerequisite learning and recommendation tasks through multi-layer perceptrons, we find PDRS consistently outperforms baseline models in all three domains, by an average margin of 7.41%. Importantly, PDRS performs especially well in cold-start scenarios with improvements of up to 17.65%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">he-etal-2022-modeling</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Modeling and Leveraging Prerequisite Context in Recommendation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Hengchang and Pan, Liangming and Ran, Yiding and Kan, Min{-}Yen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{RecSys Workshop on Context-Aware Recommender Systems (CARS@RecSys)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Seattle, WA, USA}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://arxiv.org/abs/2209.11471}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">2021</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="pan-etal-2021-zero" class="col-sm-10"> <div class="title">Zero-shot Fact Verification by Claim Generation</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2105.14682" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2021.acl-short.61.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/teacherpeterpan/Zero-shot-Fact-Verification" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/slides/qacg_slides.pdf" class="badge grey waves-effect mr-1" role="button">Slides</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:Zph67rFs4hoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-40-4285F4?logo=googlescholar&amp;labelColor=beige" alt="40 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Neural models for automated fact verification have achieved promising results thanks to the availability of large, human-annotated datasets. However, for each new domain that requires fact verification, creating a dataset by manually writing claims and linking them to their supporting evidence is expensive. We develop QACG, a framework for training a robust fact verification model by using automatically generated claims that can be supported, refuted, or unverifiable from evidence from Wikipedia. QACG generates question-answer pairs from the evidence and then converts them into different types of claims. Experiments on the FEVER dataset show that our QACG framework significantly reduces the demand for human-annotated training data. In a zero-shot scenario, QACG improves a RoBERTa model’s F1 from 50% to 77%, equivalent in performance to 2K+ manually-curated examples. Our QACG code is publicly available.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2021-zero</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Zero-shot Fact Verification by Claim Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Chen, Wenhu and Xiong, Wenhan and Kan, Min-Yen and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.acl-short.61}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{476--483}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">NAACL</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="pan-etal-2021-unsupervised" class="col-sm-10"> <div class="title">Unsupervised Multi-hop Question Answering by Question Generation</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Wenhu Chen, Wenhan Xiong, Min-Yen Kan, and William Yang Wang </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2010.12623" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://aclanthology.org/2021.naacl-main.469.mp4" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="/assets/pdf/poster/mqa-qg_poster.pdf" class="badge grey waves-effect mr-1" role="button">Poster</a> <a href="/assets/pdf/slides/mqa-qg_slides.pdf" class="badge grey waves-effect mr-1" role="button">Slides</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:5nxA0vEk-isC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-61-4285F4?logo=googlescholar&amp;labelColor=beige" alt="61 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2021-unsupervised</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Unsupervised Multi-hop Question Answering by Question Generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Chen, Wenhu and Xiong, Wenhan and Kan, Min-Yen and Wang, William Yang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2021.naacl-main.469}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{5866--5880}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">TMM</a> </abbr> </div> <div id="DBLP:journals/tmm/PanCLNKC21" class="col-sm-10"> <div class="title">A Hybrid Approach for Detecting Prerequisite Relations in Multi-Modal Food Recipes</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Jingjing Chen, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, and Tat-Seng Chua </div> <div class="periodical" style="font-size: 90%;"> <em>IEEE Transactions on Multimedia (TMM)</em>, 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/abstract/document/9288707/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Modeling the structure of culinary recipes is the core of recipe representation learning. Current approaches mostly focus on extracting the workflow graph from recipes based on text descriptions. Process images, which constitute an important part of cooking recipes, has rarely been investigated in recipe structure modeling. We study this recipe structure problem from a multi-modal learning perspective, by proposing a prerequisite tree to represent recipes with cooking images at a step-level granularity. We propose a simple-yet-effective two-stage framework to automatically construct the prerequisite tree for a recipe by (1) utilizing a trained classifier to detect pairwise prerequisite relations that fuses multi-modal features as input; then (2) applying different strategies (greedy method, maximum weight, and beam search) to build the tree structure. Experiments on the MM-ReS dataset demonstrates the advantages of introducing process images for recipe structure modeling. Also, compared with neural methods which require large numbers of training data, we show that our two-stage pipeline can achieve promising results using only 400 labeled prerequisite trees as training data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/tmm/PanCLNKC21</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Chen, Jingjing and Liu, Shaoteng and Ngo, Chong{-}Wah and Kan, Min{-}Yen and Chua, Tat{-}Seng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A Hybrid Approach for Detecting Prerequisite Relations in Multi-Modal
                      Food Recipes}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Transactions on Multimedia (TMM)}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4491--4501}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1109/TMM.2020.3042706}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">2020</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="pan-etal-2020-semantic" class="col-sm-10"> <div class="title">Semantic Graphs for Generating Deep Questions</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Yuxi Xie, Yansong Feng, Tat-Seng Chua, and Min-Yen Kan </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2004.12704" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="http://slideslive.com/38929018" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/YuxiXie/SG-Deep-Question-Generation" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:KlAtU1dfN6UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-90-4285F4?logo=googlescholar&amp;labelColor=beige" alt="90 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage. In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN). Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding. On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2020-semantic</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semantic Graphs for Generating Deep Questions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Xie, Yuxi and Feng, Yansong and Chua, Tat-Seng and Kan, Min-Yen}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.acl-main.135}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1463--1475}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="cao-etal-2020-expertise" class="col-sm-10"> <div class="title">Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen</div> <div class="author" style="font-size: 90%;"> Yixin Cao, Ruihao Shui,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Min-Yen Kan, Zhiyuan Liu, and Tat-Seng Chua </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2005.00701" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="http://slideslive.com/38929081" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://srhthu.github.io/expertise-style-transfer/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Dataset</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:MXK_kJrjxJIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-90-4285F4?logo=googlescholar&amp;labelColor=beige" alt="90 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The curse of knowledge can impede communication between experts and laymen. We propose a new task of expertise style transfer and contribute a manually annotated dataset with the goal of alleviating such cognitive biases. Solving this task not only simplifies the professional language, but also improves the accuracy and expertise level of laymen descriptions using simple words. This is a challenging task, unaddressed in previous work, as it requires the models to have expert intelligence in order to modify text with a deep understanding of domain knowledge and structures. We establish the benchmark performance of five state-of-the-art models for style transfer and text simplification. The results demonstrate a significant gap between machine and human performance. We also discuss the challenges of automatic evaluation, to provide insights into future research directions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">cao-etal-2020-expertise</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Expertise Style Transfer: A New Task Towards Better Communication between Experts and Laymen}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Cao, Yixin and Shui, Ruihao and Pan, Liangming and Kan, Min-Yen and Liu, Zhiyuan and Chua, Tat-Seng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.acl-main.100}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1061--1071}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">EMNLP</a> </abbr> </div> <div id="liu-etal-2020-exploring" class="col-sm-10"> <div class="title">Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment</div> <div class="author" style="font-size: 90%;"> Zhiyuan Liu, Yixin Cao,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Juanzi Li, Zhiyuan Liu, and Tat-Seng Chua </div> <div class="periodical" style="font-size: 90%;"> <em>In Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2010.03249" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://slideslive.com/38938987" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://github.com/thunlp/explore-and-evaluate" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:0EnyYjriUFMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-120-4285F4?logo=googlescholar&amp;labelColor=beige" alt="120 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs. GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples. However, attribute triples can also provide crucial alignment signal but have not been well explored yet. In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently. Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets. To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set. Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets. Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">liu-etal-2020-exploring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring and Evaluating Attributes, Values, and Structures for Entity Alignment}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Zhiyuan and Cao, Yixin and Pan, Liangming and Li, Juanzi and Liu, Zhiyuan and Chua, Tat-Seng}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing (EMNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Online}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.emnlp-main.515}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6355--6364}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">COLING</a> </abbr> </div> <div id="xie-etal-2020-exploring" class="col-sm-10"> <div class="title">Exploring Question-Specific Rewards for Generating Deep Questions</div> <div class="author" style="font-size: 90%;"> Yuxi Xie,  <a style="color: #0076df; font-weight: bold;">Liangming Pan<sup>*</sup></a>, Dongzhe Wang, Min-Yen Kan, and Yansong Feng </div> <div class="periodical" style="font-size: 90%;"> <em>In International Conference on Computational Linguistics (COLING)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2011.01102" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/YuxiXie/RL-for-Question-Generation" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:8k81kl-MbHgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-31-4285F4?logo=googlescholar&amp;labelColor=beige" alt="31 Google Scholar citations"> </a> <span class="periodical font-italic" style="font-size: 90%;">(*: Corresponding Author)</span> </div> <div class="abstract hidden"> <p>Recent question generation (QG) approaches often utilize the sequence-to-sequence framework (Seq2Seq) to optimize the log likelihood of ground-truth questions using teacher forcing. However, this training objective is inconsistent with actual question quality, which is often reflected by certain global properties such as whether the question can be answered by the document. As such, we directly optimize for QG-specific objectives via reinforcement learning to improve question quality. We design three different rewards that target to improve the fluency, relevance, and answerability of generated questions. We conduct both automatic and human evaluations in addition to thorough analysis to explore the effect of each QG-specific reward. We find that optimizing on question-specific rewards generally leads to better performance in automatic evaluation metrics. However, only the rewards that correlate well with human judgement (e.g., relevance) lead to real improvement in question quality. Optimizing for the others, especially answerability, introduces incorrect bias to the model, resulting in poorer question quality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">xie-etal-2020-exploring</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Exploring Question-Specific Rewards for Generating Deep Questions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Xie, Yuxi and Pan, Liangming and Wang, Dongzhe and Kan, Min-Yen and Feng, Yansong}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computational Linguistics (COLING)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Barcelona, Spain (Online)}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{International Committee on Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2020.coling-main.228}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2534--2546}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACM MM</a> </abbr> <span class="award badge">Oral Presentation</span> </div> <div id="DBLP:conf/mm/PanCWLNKJC20" class="col-sm-10"> <div class="title">Multi-modal Cooking Workflow Construction for Food Recipes</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Jingjing Chen, Jianlong Wu, Shaoteng Liu, Chong-Wah Ngo, Min-Yen Kan, Yu-Gang Jiang, and Tat-Seng Chua </div> <div class="periodical" style="font-size: 90%;"> <em>In ACM International Conference on Multimedia (ACM MM)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://arxiv.org/abs/2008.09151" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:hqOjcs7Dif8C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-28-4285F4?logo=googlescholar&amp;labelColor=beige" alt="28 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Understanding food recipe requires anticipating the implicit causal effects of cooking actions, such that the recipe can be converted into a graph describing the temporal workflow of the recipe. This is a non-trivial task that involves common-sense reasoning. However, existing efforts rely on hand-crafted features to extract the workflow graph from recipes due to the lack of large-scale labeled datasets. Moreover, they fail to utilize the cooking images, which constitute an important part of food recipes. In this paper, we build MM-ReS, the first large-scale dataset for cooking workflow construction, consisting of 9,850 recipes with human-labeled workflow graphs. Cooking steps are multi-modal, featuring both text instructions and cooking images. We then propose a neural encoder-decoder model that utilizes both visual and textual information to construct the cooking workflow, which achieved over 20% performance gain over existing hand-crafted baselines.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/mm/PanCWLNKJC20</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Chen, Jingjing and Wu, Jianlong and Liu, Shaoteng and Ngo, Chong{-}Wah and Kan, Min{-}Yen and Jiang, Yu{-}Gang and Chua, Tat{-}Seng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Multi-modal Cooking Workflow Construction for Food Recipes}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM International Conference on Multimedia (ACM MM)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1132--1141}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{ACM}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3394171.3413765}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CVPR</abbr> </div> <div id="DBLP:conf/cvpr/LiuCPNCJ20" class="col-sm-10"> <div class="title">Hyperbolic Visual Embedding Learning for Zero-Shot Recognition</div> <div class="author" style="font-size: 90%;"> Shaoteng Liu, Jingjing Chen,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Chong-Wah Ngo, Tat-Seng Chua, and Yu-Gang Jiang </div> <div class="periodical" style="font-size: 90%;"> <em>In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Hyperbolic_Visual_Embedding_Learning_for_Zero-Shot_Recognition_CVPR_2020_paper.html" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/ShaoTengLiu/Hyperbolic_ZSL" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:Se3iqnhoufwC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-130-4285F4?logo=googlescholar&amp;labelColor=beige" alt="130 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>This paper proposes a Hyperbolic Visual Embedding Learning Network for zero-shot recognition. The network learns image embeddings in hyperbolic space, which is capable of preserving the hierarchical structure of semantic classes in low dimensions. Comparing with existing zero-shot learning approaches, the network is more robust because the embedding feature in hyperbolic space better represents class hierarchy and thereby avoid misleading resulted from unrelated siblings. Our network outperforms exiting baselines under hierarchical evaluation with an extremely challenging setting, i.e., learning only from 1,000 categories to recognize 20,841 unseen categories. While under flat evaluation, it has competitive performance as state-of-the-art methods but with five times lower embedding dimensions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/cvpr/LiuCPNCJ20</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Liu, Shaoteng and Chen, Jingjing and Pan, Liangming and Ngo, Chong{-}Wah and Chua, Tat{-}Seng and Jiang, Yu{-}Gang}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hyperbolic Visual Embedding Learning for Zero-Shot Recognition}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9270--9278}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Computer Vision Foundation / {IEEE}}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://openaccess.thecvf.com/content\_CVPR\_2020/html/Liu\_Hyperbolic\_Visual\_Embedding\_Learning\_for\_Zero-Shot\_Recognition\_CVPR\_2020\_paper.html}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">AAAI</a> </abbr> </div> <div id="DBLP:conf/aaai/ChenPWWNC20" class="col-sm-10"> <div class="title">Zero-Shot Ingredient Recognition by Multi-Relational Graph Convolutional Network</div> <div class="author" style="font-size: 90%;"> Jingjing Chen,  <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Zhipeng Wei, Xiang Wang, Chong-Wah Ngo, and Tat-Seng Chua </div> <div class="periodical" style="font-size: 90%;"> <em>In AAAI Conference on Artificial Intelligence (AAAI)</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6626" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:_FxGoFyzp5QC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-51-4285F4?logo=googlescholar&amp;labelColor=beige" alt="51 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Recognizing ingredients for a given dish image is at the core of automatic dietary assessment, attracting increasing attention from both industry and academia. Nevertheless, the task is challenging due to the difficulty of collecting and labeling sufficient training data. On one hand, there are hundred thousands of food ingredients in the world, ranging from the common to rare. Collecting training samples for all of the ingredient categories is difficult. On the other hand, as the ingredient appearances exhibit huge visual variance during the food preparation, it requires to collect the training samples under different cooking and cutting methods for robust recognition. Since obtaining sufficient fully annotated training data is not easy, a more practical way of scaling up the recognition is to develop models that are capable of recognizing unseen ingredients. Therefore, in this paper, we target the problem of ingredient recognition with zero training samples. More specifically, we introduce multi-relational GCN (graph convolutional network) that integrates ingredient hierarchy, attribute as well as co-occurrence for zero-shot ingredient recognition. Extensive experiments on both Chinese and Japanese food datasets are performed to demonstrate the superior performance of multi-relational GCN and shed light on zero-shot ingredients recognition.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/aaai/ChenPWWNC20</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chen, Jingjing and Pan, Liangming and Wei, Zhipeng and Wang, Xiang and Ngo, Chong{-}Wah and Chua, Tat{-}Seng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Zero-Shot Ingredient Recognition by Multi-Relational Graph Convolutional
                      Network}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{AAAI Conference on Artificial Intelligence (AAAI)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10542--10550}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{AAAI} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1609/aaai.v34i07.6626}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h3 class="bibliography">2017</h3> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">ACL</a> </abbr> </div> <div id="pan-etal-2017-prerequisite" class="col-sm-10"> <div class="title">Prerequisite Relation Learning for Concepts in MOOCs</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Chengjiang Li, Juanzi Li, and Jie Tang </div> <div class="periodical" style="font-size: 90%;"> <em>In Annual Meeting of the Association for Computational Linguistics (ACL)</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://aclanthology.org/P17-1133/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:2osOgNQ5qMEC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-160-4285F4?logo=googlescholar&amp;labelColor=beige" alt="160 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>What prerequisite knowledge should students achieve a level of mastery before moving forward to learn subsequent coursewares? We study the extent to which the prerequisite relation between knowledge concepts in Massive Open Online Courses (MOOCs) can be inferred automatically. In particular, what kinds of information can be leverage to uncover the potential prerequisite relation between knowledge concepts. We first propose a representation learning-based method for learning latent representations of course concepts, and then investigate how different features capture the prerequisite relations between concepts. Our experiments on three datasets form Coursera show that the proposed method achieves significant improvements (+5.9-48.0% by F1-score) comparing with existing methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2017-prerequisite</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Prerequisite Relation Learning for Concepts in {MOOC}s}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Li, Chengjiang and Li, Juanzi and Tang, Jie}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics (ACL)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Vancouver, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/P17-1133}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1447--1456}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">AACL / IJCNLP</a> </abbr> </div> <div id="pan-etal-2017-course" class="col-sm-10"> <div class="title">Course Concept Extraction in MOOCs via Embedding-Based Graph Propagation</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a>, Xiaochen Wang, Chengjiang Li, Juanzi Li, and Jie Tang </div> <div class="periodical" style="font-size: 90%;"> <em>In International Joint Conference on Natural Language Processing (IJCNLP)</em>, 2017 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract badge grey waves-effect mr-1" role="button">Abstract</a> <a class="bibtex badge grey waves-effect mr-1" role="button">Bib</a> <a href="https://aclanthology.org/I17-1088/" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=JcjjOTUAAAAJ&amp;citation_for_view=JcjjOTUAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-69-4285F4?logo=googlescholar&amp;labelColor=beige" alt="69 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Massive Open Online Courses (MOOCs), offering a new way to study online, are revolutionizing education. One challenging issue in MOOCs is how to design effective and fine-grained course concepts such that students with different backgrounds can grasp the essence of the course. In this paper, we conduct a systematic investigation of the problem of course concept extraction for MOOCs. We propose to learn latent representations for candidate concepts via an embedding-based method. Moreover, we develop a graph-based propagation algorithm to rank the candidate concepts based on the learned representations. We evaluate the proposed method using different courses from XuetangX and Coursera. Experimental results show that our method significantly outperforms all the alternative methods (+0.013-0.318 in terms of R-precision; p\textless\textless0.01, t-test).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pan-etal-2017-course</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Course Concept Extraction in {MOOC}s via Embedding-Based Graph Propagation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pan, Liangming and Wang, Xiaochen and Li, Chengjiang and Li, Juanzi and Tang, Jie}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Joint Conference on Natural Language Processing (IJCNLP)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Taipei, Taiwan}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Asian Federation of Natural Language Processing}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/I17-1088}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{875--884}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">PhD thesis</h2> <h3 class="bibliography">2022</h3> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0076df"> <a href="">PhD thesis</a> </abbr> </div> <div id="phdthesis-pan-2022-QG" class="col-sm-10"> <div class="title">Towards Generating Deep Questions from Text</div> <div class="author" style="font-size: 90%;"> <a style="color: #0076df; font-weight: bold;">Liangming Pan</a> </div> <div class="periodical" style="font-size: 90%;"> <em>National University of Singapore</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a href="https://www.proquest.com/docview/2723857718?pq-origsite=gscholar&amp;fromopenview=true&amp;sourcetype=Dissertations%20&amp;%20Theses" class="badge grey waves-effect mr-1" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Liangming Pan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-home",title:"Home",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-bio",title:"Bio",description:"You can find a PDF version of my full CV on the right.",section:"Navigation",handler:()=>{window.location.href="/bio/"}},{id:"nav-publications",title:"Publications",description:"* denotes equal contribution",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-talks-amp-teaching",title:"Talks &amp; Teaching",description:"",section:"Navigation",handler:()=>{window.location.href="/talks_teaching/"}},{id:"nav-service",title:"Service",description:"",section:"Navigation",handler:()=>{window.location.href="/service/"}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:'Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:'Displaying External Posts on Your al-folio Blog <svg width="1.2rem" height="1.2rem" top=".5rem" viewBox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"><path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg>',description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-github-metadata",title:"a post with github metadata",description:"a quick run down on accessing github metadata.",section:"Posts",handler:()=>{window.location.href="/blog/2020/github-metadata/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-i-will-serve-as-an-area-chair-for-the-question-answering-track-of-acl-2023-starting-from-dec-2022-i-am-thrilled-to-join-the-uc-santa-barbara-natural-language-processing-group-as-a-postdoctoral-scholar-working-with-prof-william-yang-wang",title:"I will serve as an Area Chair for the Question Answering track of...",description:"",section:"News"},{id:"news-our-paper-hashtag-guided-low-resource-tweet-classification-was-accepted-by-www-2023-invited-talk-building-data-efficient-and-explainable-fact-checking-models-at-mohamed-bin-zayed-university-of-artificial-intelligence-mbzuai",title:"Our paper Hashtag-Guided Low-Resource Tweet Classification was accepted by WWW 2023. Invited Talk...",description:"",section:"News"},{id:"news-new-preprint-logic-lm-empowering-large-language-models-with-symbolic-solvers-for-faithful-logical-reasoning-our-paper-fact-checking-complex-claims-with-program-guided-reasoning-was-accepted-by-the-main-conference-of-acl-2023",title:"New Preprint! Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical...",description:"",section:"News"},{id:"news-new-survey-paper-automatically-correcting-large-language-models-surveying-the-landscape-of-diverse-self-correction-strategies-we-also-create-a-paper-list",title:"New Survey Paper! Automatically Correcting Large Language Models: Surveying the landscape of diverse...",description:"",section:"News"},{id:"news-i-have-8-papers-accepted-by-emnlp-2023-4-main-conference-3-findings-1-demo-topics-involves-logical-reasoning-safety-and-evaluation-of-llms",title:"I have 8 papers accepted by EMNLP 2023 (4 Main Conference, 3 Findings,...",description:"",section:"News"},{id:"news-our-paper-attacking-open-domain-question-answering-by-injecting-misinformation-received-the-area-chair-award-question-answering-at-ijcnlp-aacl-2023",title:"Our paper Attacking Open-domain Question Answering by Injecting Misinformation received the Area Chair...",description:"",section:"News"},{id:"news-invited-talk-combating-misinformation-in-the-age-of-llms-at-nus-centre-for-trusted-internet-and-community-slides",title:"Invited talk \u201cCombating Misinformation in the age of LLMs\u201d at NUS Centre for...",description:"",section:"News"},{id:"news-i-will-serve-as-the-student-volunteer-chair-and-an-area-chair-of-acl-2024",title:"I will serve as the Student Volunteer Chair and an Area Chair of...",description:"",section:"News"},{id:"news-6-papers-accepted-by-acl-2024-2-main-conference-4-findings-topics-involves-logical-reasoning-uncertainty-estimation-and-self-correction-of-llms-2-papers-accepted-by-icml-2024-one-paper-is-on-understanding-the-chain-of-thought-reasoning-ability-of-llms-the-other-paper-is-on-analyzing-the-impact-of-social-media-influencers-on-ai-research-visibility",title:"6 papers accepted by ACL 2024 (2 Main Conference, 4 Findings). Topics involves...",description:"",section:"News"},{id:"news-invited-talk-empowering-large-language-models-with-faithful-reasoning-at-tsinghua-university-peking-university-xi-an-jiaotong-university-and-harbin-institute-of-technology-shenzhen",title:"Invited talk \u201cEmpowering Large Language Models with Faithful Reasoning\u201d at Tsinghua University, Peking...",description:"",section:"News"},{id:"news-i-will-serve-as-an-area-chair-for-emnlp-2024-and-coling-2025",title:"I will serve as an Area Chair for EMNLP 2024 and COLING 2025....",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%69%61%6E%67%6D%69%6E%67%70%61%6E@%75%63%73%62.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=JcjjOTUAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/Liangming-Pan/3470231","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/teacherpeterpan","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/liangmingpan/en","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/PanLiangming","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>