---
---

@string{tacl = {Transactions of the Association for Computational Linguistics (TACL)}}
@string{emnlp = {Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{acl = {Annual Meeting of the Association for Computational Linguistics (ACL)}}
@string{emnlpfindings = {Findings of Conference on Empirical Methods in Natural Language Processing (EMNLP)}}
@string{aclfindings = {Findings of Annual Meeting of the Association for Computational Linguistics (ACL)}}
@string{icml = {International Conference on Machine Learning (ICML)}}
@string{tmlr = {Transactions on Machine Learning Research (TMLR)}}

@misc{ma2024mmlongbenchdocbenchmarkinglongcontextdocument,
    abbr = {arXiv},
    dataset={https://huggingface.co/datasets/yubo2333/MMLongBench-Doc},
    code={https://github.com/mayubo2333/MMLongBench-Doc},
    website={https://mayubo2333.github.io/MMLongBench-Doc/},
    pdf={https://arxiv.org/abs/2407.01523},
    abstract = "Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLongBench-Doc, a long-context, multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct from previous datasets, it is constructed upon 130 lengthy PDF-formatted documents with an average of 49.4 pages and 20,971 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e. page number). Moreover, 33.2% of the questions are cross-page questions requiring evidence across multiple pages. 22.8% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores 31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.",
    bibtex_show = {true},
    title={MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations}, 
    author={Yubo Ma and Yuhang Zang and Liangyu Chen and Meiqi Chen and Yizhu Jiao and Xinze Li and Xinyuan Lu and Ziyu Liu and Yan Ma and Xiaoyi Dong and Pan Zhang and Liangming Pan and Yu-Gang Jiang and Jiaqi Wang and Yixin Cao and Aixin Sun},
    year={2024},
    eprint={2407.01523},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2407.01523}
}

@misc{yao2024seakrselfawareknowledgeretrieval,
    abbr = {arXiv},
    code={https://github.com/thu-keg/seakr},
    pdf={https://arxiv.org/abs/2406.19215},
    abstract = "This paper introduces Self-aware Knowledge Retrieval (SeaKR), a novel adaptive RAG model that extracts self-aware uncertainty of LLMs from their internal states. SeaKR activates retrieval when the LLMs present high self-aware uncertainty for generation. To effectively integrate retrieved knowledge snippets, SeaKR re-ranks them based on LLM's self-aware uncertainty to preserve the snippet that reduces their uncertainty to the utmost. To facilitate solving complex tasks that require multiple retrievals, SeaKR utilizes their self-aware uncertainty to choose among different reasoning strategies. Our experiments on both complex and simple Question Answering datasets show that SeaKR outperforms existing adaptive RAG methods.",
    bibtex_show = {true},
    title={SeaKR: Self-aware Knowledge Retrieval for Adaptive Retrieval Augmented Generation}, 
    author={Zijun Yao and Weijian Qi and Liangming Pan and Shulin Cao and Linmei Hu and Weichuan Liu and Lei Hou and Juanzi Li},
    year={2024},
    eprint={2406.19215},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.19215}
}

@misc{wong2024distilrrtransferringcoderepair,
    abbr = {arXiv},
    code={https://github.com/kylewong288/distilrr},
    pdf={https://arxiv.org/abs/2406.14867},
    abstract = "Large language models (LLMs) have shown remarkable performance on code generation tasks. A recent application of LLMs for code generation is iterative code repair, where a model fixes an incorrect program by rationalizing about errors and generating a new program. However, code repair is primarily studied on high-resource languages like Python, and the framework's efficacy is under-explored on low-resource languages. To apply code repair for low-resource languages, we propose Distilling Low-Resource Repairs (DistiLRR), an approach that transfers the reasoning and code generation ability from a teacher model to a student model. Our results show that DistiLRR consistently outperforms baselines on low-resource languages, but has similar performance on high-resource languages. To investigate this behavior, we perform a further analysis and find that the correlation between rationale quality and code correctness is weaker than previously perceived. We hypothesize this weakness is magnified in low-resource settings where base models lack deep knowledge of a programming language, leading to wavering benefits of code repair between high-resource and low-resource languages.",
    bibtex_show = {true},
    title={DistiLRR: Transferring Code Repair for Low-Resource Programming Languages}, 
    author={Kyle Wong and Alfonso Amayuelas and Liangming Pan and William Yang Wang},
    year={2024},
    eprint={2406.14867},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2406.14867}
}

@misc{amayuelas2024multiagentcollaborationattackinvestigating,
    abbr = {arXiv},
    pdf={https://arxiv.org/abs/2406.14711},
    abstract = "Large Language Models (LLMs) have shown exceptional results on current benchmarks when working individually. The advancement in their capabilities, along with a reduction in parameter size and inference times, has facilitated the use of these models as agents, enabling interactions among multiple models to execute complex tasks. Such collaborations offer several advantages, including the use of specialized models (e.g. coding), improved confidence through multiple computations, and enhanced divergent thinking, leading to more diverse outputs. Thus, the collaborative use of language models is expected to grow significantly in the coming years. In this work, we evaluate the behavior of a network of models collaborating through debate under the influence of an adversary. We introduce pertinent metrics to assess the adversary's effectiveness, focusing on system accuracy and model agreement. Our findings highlight the importance of a model's persuasive ability in influencing others. Additionally, we explore inference-time methods to generate more compelling arguments and evaluate the potential of prompt-based mitigation as a defensive strategy.",
    bibtex_show = {true},
    title={MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate}, 
    author={Alfonso Amayuelas and Xianjun Yang and Antonis Antoniades and Wenyue Hua and Liangming Pan and William Wang},
    year={2024},
    eprint={2406.14711},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2406.14711}
}

@misc{wu2024updatinglanguagemodelsunstructured,
    abbr = {arXiv},
    pdf={https://arxiv.org/abs/2402.18909},
    abstract = "Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further show that this challenge persists even if we extract triplets as structured facts. Our analysis discloses key insights to motivate future research in UKE for more practical knowledge editing.",
    bibtex_show = {true},
    title={Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing}, 
    author={Xiaobao Wu and Liangming Pan and William Yang Wang and Anh Tuan Luu},
    year={2024},
    eprint={2402.18909},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2402.18909}
}

@misc{ma2024sciagenttoolaugmentedlanguagemodels,
    abbr = {arXiv},
    pdf={https://arxiv.org/abs/2402.11451},
    abstract = "Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the same size by more than 13% in absolute accuracy. Furthermore, SciAgent-DeepMath-7B shows much superior performance than ChatGPT.",
    bibtex_show = {true},
    title={SciAgent: Tool-augmented Language Models for Scientific Reasoning}, 
    author={Yubo Ma and Zhibin Gou and Junheng Hao and Ruochen Xu and Shuohang Wang and Liangming Pan and Yujiu Yang and Yixin Cao and Aixin Sun and Hany Awadalla and Weizhu Chen},
    year={2024},
    eprint={2402.11451},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2402.11451}
}

@misc{yang2023surveydetectionllmsgeneratedcontent,
    abbr = {arXiv},
    google_scholar_id = {JV2RwH3_ST0C},
    website={https://github.com/xianjun-yang/awesome_papers_on_llms_detection},
    pdf={https://arxiv.org/abs/2310.15654},
    abstract = "The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content.",
    bibtex_show = {true},
    selected = {true},
    title={A Survey on Detection of LLMs-Generated Content}, 
    author={Xianjun Yang and Liangming Pan and Xuandong Zhao and Haifeng Chen and Linda Petzold and William Yang Wang and Wei Cheng},
    year={2023},
    eprint={2310.15654},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2310.15654}
}

@misc{wang2024factcheckbenchfinegrainedevaluationbenchmark,
    abbr = {arXiv},
    google_scholar_id = {hMod-77fHWUC},
    code={https://github.com/yuxiaw/factcheck-gpt},
    pdf={https://arxiv.org/abs/2311.09000},
    abstract = "The increased use of large language models (LLMs) across a variety of real-world applications calls for mechanisms to verify the factual accuracy of their outputs. In this work, we present a holistic end-to-end solution for annotating the factuality of LLM-generated responses, which encompasses a multi-stage annotation scheme designed to yield detailed labels concerning the verifiability and factual inconsistencies found in LLM outputs. We design and build an annotation tool to speed up the labelling procedure and ease the workload of raters. It allows flexible incorporation of automatic results in any stage, e.g. automatically-retrieved evidence. We further construct an open-domain document-level factuality benchmark in three-level granularity: claim, sentence and document. Preliminary experiments show that FacTool, FactScore and Perplexity.ai are struggling to identify false claims with the best F1=0.53.",
    bibtex_show = {true},
    title={Factcheck-Bench: Fine-Grained Evaluation Benchmark for Automatic Fact-checkers}, 
    author={Yuxia Wang and Revanth Gangi Reddy and Zain Muhammad Mujahid and Arnav Arora and Aleksandr Rubashevskii and Jiahui Geng and Osama Mohammed Afzal and Liangming Pan and Nadav Borenstein and Aditya Pillai and Isabelle Augenstein and Iryna Gurevych and Preslav Nakov},
    year={2024},
    eprint={2311.09000},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2311.09000}
}

@article{pan-etal-2024-automatically,
    abbr = {TACL},
    google_scholar_id={vV6vV6tmYwMC},
    website={https://github.com/teacherpeterpan/self-correction-llm-papers},
    pdf={https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00660/120911},
    abstract = "While large language models (LLMs) have shown remarkable effectiveness in various NLP tasks, they are still prone to issues such as hallucination, unfaithful reasoning, and toxicity. A promising approach to rectify these flaws is correcting LLMs with feedback, where the LLM itself is prompted or guided with feedback to fix problems in its own output. Techniques leveraging automated feedback{---}either produced by the LLM itself (self-correction) or some external system{---}are of particular interest as they make LLM-based solutions more practical and deployable with minimal human intervention. This paper provides an exhaustive review of the recent advances in correcting LLMs with automated feedback, categorizing them into training-time, generation-time, and post-hoc approaches. We also identify potential challenges and future directions in this emerging field.",
    bibtex_show = {true},
    selected = {true},
    title = "Automatically Correcting Large Language Models: Surveying the Landscape of Diverse Automated Correction Strategies",
    author = "Pan, Liangming  and
      Saxon, Michael  and
      Xu, Wenda  and
      Nathani, Deepak  and
      Wang, Xinyi  and
      Wang, William Yang",
    journal = tacl,
    volume = "12",
    year = "2024",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2024.tacl-1.27",
    pages = "484--506"
}

@inproceedings{xu-etal-2024-faithful,
    abbr = {ACL},
    code={https://github.com/aiden0526/symbcot},
    pdf={https://arxiv.org/abs/2405.18357},
    bibtex_show = {true},
    selected = {true},
    abstract = "While the recent Chain-of-Thought (CoT) technique enhances the reasoning ability of large language models (LLMs) with the theory of mind, it might still struggle in handling logical reasoning that relies much on symbolic expressions and rigid deducing rules. To strengthen the logical reasoning capability of LLMs, we propose a novel Symbolic Chain-of-Thought, namely SymbCoT, a fully LLM-based framework that integrates symbolic expressions and logic rules with CoT prompting. Technically, building upon an LLM, SymbCoT 1) first translates the natural language context into the symbolic format, and then 2) derives a step-by-step plan to solve the problem with symbolic logical rules, 3) followed by a verifier to check the translation and reasoning chain. Via thorough evaluations on 5 standard datasets with both First-Order Logic and Constraint Optimization symbolic expressions, SymbCoT shows striking improvements over the CoT method consistently, meanwhile refreshing the current state-of-the-art performances. We further demonstrate that our system advances in more faithful, flexible, and explainable logical reasoning. To our knowledge, this is the first to combine symbolic expressions and rules into CoT for logical reasoning with LLMs.",
    title = "Faithful Logical Reasoning via Symbolic Chain-of-Thought",
    author = {Jundong Xu and
              Hao Fei and
              Liangming Pan and
              Qian Liu and
              Mong{-}Li Lee and
              Wynne Hsu},
    booktitle = acl,
    year = "2024",
    address = "Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2405.18357"
}

@inproceedings{xu-etal-2024-pride,
    abbr = {ACL},
    google_scholar_id={3s1wT3WcHBgC},
    code={https://github.com/xu1998hz/llm_self_bias},
    pdf={https://arxiv.org/abs/2402.11436},
    bibtex_show = {true},
    oral = {true},
    selected = {true},
    abstract = "Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.",
    title = "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
    author = {Wenda Xu and
              Guanglei Zhu and
              Xuandong Zhao and
              Liangming Pan and
              Lei Li and
              William Yang Wang},
    booktitle = acl,
    year = "2024",
    address = "Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2402.11436"
}

@inproceedings{alfonso-etal-2024-knowledge,
    abbr = {ACL},
    code={https://github.com/amayuelas/knowledge-of-knowledge},
    google_scholar_id={TQgYirikUcIC},
    pdf={https://arxiv.org/abs/2305.13712},
    bibtex_show = {true},
    selected = {true},
    abstract = "This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their knowledge and uncertainty over questions. Specifically, we focus on addressing known-unknown questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a new dataset with Known-Unknown Questions (KUQ) and establish a categorization framework to clarify the origins of uncertainty in such queries. Subsequently, we examine the performance of open-source LLMs, fine-tuned using this dataset, in distinguishing between known and unknown queries within open-ended question-answering scenarios. The fine-tuned models demonstrated a significant improvement, achieving a considerable increase in F1-score relative to their pre-fine-tuning state. Through a comprehensive analysis, we reveal insights into the models' improved uncertainty articulation and their consequent efficacy in multi-agent debates. These findings help us understand how LLMs can be trained to identify and express uncertainty, improving our knowledge of how they understand and express complex or unclear information.",
    title = "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
    author={Alfonso Amayuelas and Kyle Wong and Liangming Pan and Wenhu Chen and William Yang Wang},
    booktitle = aclfindings,
    year = "2024",
    address = "Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2305.13712"
}

@inproceedings{zhang-etal-2024-knowledge,
    abbr = {ACL},
    code={https://github.com/shuozhangxjtu/mixalign},
    google_scholar_id={M05iB0D1s5AC},
    pdf={https://arxiv.org/abs/2305.13669},
    bibtex_show = {true},
    selected = {true},
    abstract = "Large language models often necessitate grounding on external knowledge to generate faithful and reliable answers. Yet even with the correct groundings in the reference, they can ignore them and rely on wrong groundings or their inherent biases to hallucinate when users, being largely unaware of the specifics of the stored information, pose questions that might not directly correlate with the retrieved groundings. In this work, we formulate this knowledge alignment problem and introduce MixAlign, a framework that interacts with both the human user and the knowledge base to obtain and integrate clarifications on how the user question relates to the stored information. MixAlign employs a language model to achieve automatic knowledge alignment and, if necessary, further enhances this alignment through human user clarifications. Experimental results highlight the crucial role of knowledge alignment in boosting model performance and mitigating hallucination, with improvements noted up to 22.2% and 27.1% respectively. We also demonstrate the effectiveness of MixAlign in improving knowledge alignment by producing high-quality, user-centered clarifications.",
    title = "The Knowledge Alignment Problem: Bridging Human and External Knowledge for Large Language Models",
    author={Shuo Zhang and Liangming Pan and Junzhou Zhao and William Yang Wang},
    booktitle = aclfindings,
    year = "2024",
    address = "Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2305.13669"
}

@inproceedings{li-etal-2024-towards,
    abbr = {ACL},
    pdf={https://arxiv.org/abs/2310.05634},
    bibtex_show = {true},
    abstract = "Although achieving great success, Large Language Models (LLMs) usually suffer from unreliable hallucinations. Although language attribution can be a potential solution, there are no suitable benchmarks and evaluation metrics to attribute LLMs to structured knowledge. In this paper, we define a new task of Knowledge-aware Language Model Attribution (KaLMA) that improves upon three core concerns with conventional attributed LMs. First, we extend attribution source from unstructured texts to Knowledge Graph (KG), whose rich structures benefit both the attribution performance and working scenarios. Second, we propose a new ``Conscious Incompetence'' setting considering the incomplete knowledge repository, where the model identifies the need for supporting knowledge beyond the provided KG. Third, we propose a comprehensive automatic evaluation metric encompassing text quality, citation quality, and text citation alignment. To implement the above innovations, we build a dataset in biography domain BioKaLMA via evolutionary question generation strategy, to control the question complexity and necessary knowledge to the answer. For evaluation, we develop a baseline solution and demonstrate the room for improvement in LLMs' citation generation, emphasizing the importance of incorporating the ``Conscious Incompetence'' setting, and the critical role of retrieval accuracy.",
    title = "Towards Verifiable Generation: A Benchmark for Knowledge-aware Language Model Attribution",
    author={Xinze Li and Yixin Cao and Liangming Pan and Yubo Ma and Aixin Sun},
    booktitle = aclfindings,
    year = "2024",
    address = "Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2310.05634"
}

@inproceedings{wu-etal-2024-modeling,
    abbr = {ACL},
    code={https://github.com/bobxwu/CFDTM},
    pdf={https://arxiv.org/abs/2405.17957},
    bibtex_show = {true},
    abstract = "Dynamic topic models track the evolution of topics in sequential documents, which have derived various applications like trend analysis and opinion mining. However, existing models suffer from repetitive topic and unassociated topic issues, failing to reveal the evolution and hindering further applications. To address these issues, we break the tradition of simply chaining topics in existing work and propose a novel neural \modelfullname. We introduce a new evolution-tracking contrastive learning method that builds the similarity relations among dynamic topics. This not only tracks topic evolution but also maintains topic diversity, mitigating the repetitive topic issue. To avoid unassociated topics, we further present an unassociated word exclusion method that consistently excludes unassociated words from discovered topics. Extensive experiments demonstrate our model significantly outperforms state-of-the-art baselines, tracking topic evolution with high-quality topics, showing better performance on downstream tasks, and remaining robust to the hyperparameter for evolution intensities.",
    title = "Modeling Dynamic Topics in Chain-Free Fashion by Evolution-Tracking Contrastive Learning and Unassociated Word Exclusion",
    author={Xiaobao Wu and Xinshuai Dong and Liangming Pan and Thong Nguyen and Anh Tuan Luu},
    booktitle = aclfindings,
    year = "2024",
    address = "Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://arxiv.org/abs/2405.17957"
}

@inproceedings{wang-etal-2024-understanding,
    abbr = {ICML},
    code={https://github.com/wangxinyilinda/lm_random_walk},
    pdf={https://arxiv.org/abs/2402.03268},
    bibtex_show = {true},
    selected = {true},
    abstract = "Pre-trained language models (LMs) are able to perform complex reasoning without explicit fine-tuning. To understand how pre-training with a next-token prediction objective contributes to the emergence of such reasoning capability, we propose that we can view an LM as deriving new conclusions by aggregating indirect reasoning paths seen at pre-training time. We found this perspective effective in two important cases of reasoning: logic reasoning with knowledge graphs (KGs) and chain-of-thought (CoT) reasoning. More specifically, we formalize the reasoning paths as random walk paths on the knowledge/reasoning graphs. Analyses of learned LM distributions suggest that a weighted sum of relevant random walk path probabilities is a reasonable way to explain how LMs reason. Experiments and analysis on multiple KG and CoT datasets reveal the effect of training on random walk paths and suggest that augmenting unlabeled random walk reasoning paths can improve real-world multi-step reasoning performance.",
    title = "Understanding Reasoning Ability of Language Models From the Perspective of Reasoning Paths Aggregation",
    author={Xinyi Wang and Alfonso Amayuelas and Kexun Zhang and Liangming Pan and Wenhu Chen and William Yang Wang},
    booktitle = icml,
    year = "2024",
    address = "Austria",
    url = "https://arxiv.org/abs/2402.03268"
}

@inproceedings{weissburg-etal-2024-tweets,
    abbr = {ICML},
    pdf={https://arxiv.org/abs/2401.13782},
    bibtex_show = {true},
    abstract = "As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside controls precisely matched by 9 key covariates. Our statistical and causal inference analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. Given these findings, we advocate for a responsible approach to curation, encouraging influencers to uphold the journalistic standard that includes showcasing diverse research topics, authors, and institutions.",
    title = "Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility",
    author={Iain Xie Weissburg and Mehir Arora and Xinyi Wang and Liangming Pan and William Yang Wang},
    booktitle = icml,
    year = "2024",
    address = "Austria",
    url = "https://arxiv.org/abs/2401.13782"
}

@article{DBLP:journals/corr/abs-2402-16827,
  abbr = {TMLR},
  google_scholar_id={J_g5lzvAfSwC},
  website={https://github.com/alon-albalak/data-selection-survey},
  pdf={https://arxiv.org/abs/2402.16827},
  abstract={A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required. Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies. To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.},
  bibtex_show = {true},
  author       = {Alon Albalak and
                  Yanai Elazar and
                  Sang Michael Xie and
                  Shayne Longpre and
                  Nathan Lambert and
                  Xinyi Wang and
                  Niklas Muennighoff and
                  Bairu Hou and
                  Liangming Pan and
                  Haewon Jeong and
                  Colin Raffel and
                  Shiyu Chang and
                  Tatsunori Hashimoto and
                  William Yang Wang},
  title        = {A Survey on Data Selection for Language Models},
  journal      = tmlr,
  year         = {2024},
  url          = {https://doi.org/10.48550/arXiv.2402.16827}
}

@inproceedings{lu-etal-2023-scitab,
    abbr = {EMNLP},
    google_scholar_id={hFOr9nPyWt4C},
    code={https://github.com/xinyuanlu00/scitab},
    pdf={https://arxiv.org/abs/2305.13186},
    video = {https://aclanthology.org/2023.emnlp-main.483.mp4},
    abstract = "Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are publicly available at https://github.com/XinyuanLu00/SciTab.",
    bibtex_show = {true},
    selected = {true},
    title = "{SCITAB}: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables",
    author = "Lu*, Xinyuan  and
      Pan*, Liangming  and
      Liu, Qian  and
      Nakov, Preslav  and
      Kan, Min-Yen",
    booktitle = emnlp,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.483",
    pages = "7787--7813"
}

@inproceedings{nathani-etal-2023-maf,
    abbr = {EMNLP},
    google_scholar_id={maZDTaKrznsC},
    code={https://github.com/deepakn97/maf},
    pdf={https://arxiv.org/abs/2310.12426},
    video={https://aclanthology.org/2023.emnlp-main.407.mp4},
    abstract = "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through *self-improvement* using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose **Multi-Aspect Feedback**, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see an improvement of up to 20{\%} in Mathematical Reasoning and up to 18{\%} in Logical Entailment.",
    bibtex_show = {true},
    selected = {true},
    title = "{MAF}: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
    author = "Nathani, Deepak  and
      Wang, David  and
      Pan, Liangming  and
      Wang, William",
    booktitle = emnlp,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.407",
    pages = "6591--6616"
}

@inproceedings{xu-etal-2023-instructscore,
    abbr = {EMNLP},
    google_scholar_id={ldfaerwXgEUC},
    code={https://github.com/xu1998hz/sescore3},
    pdf={https://arxiv.org/abs/2305.14282},
    video={https://aclanthology.org/2023.emnlp-main.365.mp4},
    model={https://huggingface.co/xu1998hz/InstructScore},
    bibtex_show = {true},
    selected = {true},
    abstract = "Automatically evaluating the quality of language generation is critical. Although recent learned metrics show high correlation with human judgement, these metrics do not provide explicit explanation of their verdict, nor associate the scores with defects in the generated text. To address this limitation, we present INSTRUCTSCORE, a fine-grained explainable evaluation metric for text generation. By harnessing both explicit human instruction and the implicit knowledge of GPT-4, we fine-tune a text evaluation metric based on LLaMA, producing both a score for generated text and a human readable diagnostic report. We evaluate INSTRUCTSCORE on a variety of generation tasks, including translation, captioning, data-to-text, and commonsense generation. Experiments show that our 7B model surpasses all other unsupervised metrics, including those based on 175B GPT-3 and GPT-4. Surprisingly, our INSTRUCTSCORE, even without direct supervision from human-rated data, achieves performance levels on par with state-of-the-art metrics like COMET22, which were fine-tuned on human ratings.",
    title = "{INSTRUCTSCORE}: Towards Explainable Text Generation Evaluation with Automatic Feedback",
    author = "Xu, Wenda  and
      Wang, Danqing  and
      Pan, Liangming  and
      Song, Zhenqiao  and
      Freitag, Markus  and
      Wang, William  and
      Li, Lei",
    booktitle = emnlp,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.365",
    pages = "5967--5994"
}

@inproceedings{diao-etal-2023-doolittle,
    abbr = {EMNLP},
    dataset={https://github.com/shizhediao/Doolittle},
    pdf={https://aclanthology.org/2023.emnlp-main.809},
    video={https://aclanthology.org/2023.emnlp-main.809.mp4},
    bibtex_show = {true},
    abstract = "Improving the quality of academic writing is a meaningful but challenging task. Conventional methods of language refinement focus on narrow, specific linguistic features within isolated sentences, such as grammatical errors and improper word use. We propose a more general task, Academic Writing Formalization (AWF), to improve the overall quality of formal academic writing at the paragraph level. We formulate this language refinement task as a formal text style transfer task which transfers informal-academic text to formal-academic and contribute a large-scale non-parallel dataset, Doolittle, for this purpose. Concurrently, we apply a method named metric-oriented reinforcement learning (MORL) to two large language models (LLM) where we incorporate different levels of automatic feedback into the training process. Our experiments reveal that existing text transfer models and grammatical error correction models address certain aspects of AWF but still have a significant performance gap compared to human performance. Meanwhile, language models fine-tuned with our MORL method exhibit considerably improved performance, rivaling the latest chatbot ChatGPT, but still have a non-negligible gap compared to the ground truth formal-academic texts in Doolittle.",
    title = "Doolittle: Benchmarks and Corpora for Academic Writing Formalization",
    author = "Diao, Shizhe  and
      Lei, Yongyu  and
      Pan, Liangming  and
      Fang, Tianqing  and
      Zhou, Wangchunshu  and
      Keh, Sedrick  and
      Kan, Min-Yen  and
      Zhang, Tong",
    booktitle = emnlp,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.809",
    pages = "13093--13111"
}

@inproceedings{pan-etal-2023-logic,
    abbr = {EMNLP},
    google_scholar_id={RHpTSmoSYBkC},
    code={https://github.com/teacherpeterpan/logic-llm},
    pdf={https://arxiv.org/abs/2305.12295},
    bibtex_show = {true},
    selected = {true},
    abstract = "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver{'}s error messages to revise symbolic formalizations. We demonstrate Logic-LM{'}s effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2{\%} over using LLM alone with standard prompting and 18.4{\%} over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning.",
    title = "Logic-{LM}: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
    author = "Pan, Liangming  and
      Albalak, Alon  and
      Wang, Xinyi  and
      Wang, William",
    booktitle = emnlpfindings,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.248",
    pages = "3806--3824"
}

@inproceedings{pan-etal-2023-risk,
    abbr = {EMNLP},
    google_scholar_id={mB3voiENLucC},
    code={https://github.com/MexicanLemonade/LLM-Misinfo-QA},
    pdf={https://arxiv.org/abs/2305.13661},
    abstract = "We investigate the potential misuse of modern Large Language Models (LLMs) for generating credible-sounding misinformation and its subsequent impact on information-intensive applications, particularly Open-Domain Question Answering (ODQA) systems. We establish a threat model and simulate potential misuse scenarios, both unintentional and intentional, to assess the extent to which LLMs can be utilized to produce misinformation. Our study reveals that LLMs can act as effective misinformation generators, leading to a significant degradation (up to 87{\%}) in the performance of ODQA systems. Moreover, we uncover disparities in the attributes associated with persuading humans and machines, presenting an obstacle to current human-centric approaches to combat misinformation. To mitigate the harm caused by LLM-generated misinformation, we propose three defense strategies: misinformation detection, vigilant prompting, and reader ensemble. These approaches have demonstrated promising results, albeit with certain associated costs. Lastly, we discuss the practicality of utilizing LLMs as automatic misinformation generators and provide relevant resources and code to facilitate future research in this area.",
    bibtex_show = {true},
    selected = {true},
    title = "On the Risk of Misinformation Pollution with Large Language Models",
    author = "Pan*, Yikang and
      Pan*, Liangming and
      Chen, Wenhu  and
      Nakov, Preslav  and
      Kan, Min-Yen  and
      Wang, William",
    booktitle = emnlpfindings,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.97",
    pages = "1389--1403"
}

@inproceedings{pan-etal-2023-qacheck,
    abbr = {EMNLP},
    code={https://github.com/xinyuanlu00/qacheck},
    pdf={https://arxiv.org/abs/2310.07609},
    video={https://aclanthology.org/2023.emnlp-demo.23.mp4},
    bibtex_show = {true},
    selected = {true},
    abstract = "Fact-checking real-world claims often requires intricate, multi-step reasoning due to the absence of direct evidence to support or refute them. However, existing fact-checking systems often lack transparency in their decision-making, making it challenging for users to comprehend their reasoning process. To address this, we propose the Question-guided Multi-hop Fact-Checking (QACheck) system, which guides the model{'}s reasoning process by asking a series of questions critical for verifying a claim. QACheck has five key modules: a claim verifier, a question generator, a question-answering module, a QA validator, and a reasoner. Users can input a claim into QACheck, which then predicts its veracity and provides a comprehensive report detailing its reasoning process, guided by a sequence of (question, answer) pairs. QACheck also provides the source of evidence supporting each question, fostering a transparent, explainable, and user-friendly fact-checking process.",
    title = "{QAC}heck: A Demonstration System for Question-Guided Multi-Hop Fact-Checking",
    author = "Pan, Liangming  and
      Lu, Xinyuan  and
      Kan, Min-Yen  and
      Nakov, Preslav",
    booktitle = "Conference on Empirical Methods in Natural Language Processing: System Demonstrations (EMNLP Demo)",
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-demo.23",
    pages = "264--273"
}

@inproceedings{weissburg-etal-2024-tweets,
    abbr = {NeurIPS},
    google_scholar_id={GnPB-g6toBAC},
    pdf={https://arxiv.org/abs/2312.02406},
    bibtex_show = {true},
    selected = {true},
    award = {Spotlight Paper},
    abstract = "The data used to pretrain large language models has a decisive impact on a model's downstream performance, which has led to a large body of work on data selection methods that aim to automatically determine the most suitable data to use for pretraining. Existing data selection methods suffer from slow and computationally expensive processes, a problem amplified by the increasing size of models and of pretraining datasets. Data mixing, on the other hand, reduces the complexity of data selection by grouping data points together and determining sampling probabilities across entire groups. However, data mixing proportions are typically fixed before training and therefore cannot adapt to changing training dynamics. To address these limitations, we develop an efficient algorithm for Online Data Mixing (ODM) that combines elements from both data selection and data mixing. Based on multi-armed bandit algorithms, our online approach optimizes the data mixing proportions during training. Remarkably, our method trains a model that reaches the final perplexity of the next best method with 19\% fewer training iterations, and improves performance on the 5-shot MMLU benchmark by 1.9% relative accuracy, while adding negligible wall-clock time during pretraining.",
    title = "Efficient Online Data Mixing For Language Model Pre-Training",
    author={Alon Albalak and Liangming Pan and Colin Raffel and William Yang Wang},
    booktitle = {NeurIPS Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models (R0-FoMo@NeurIPS)},
    year = "2023",
    address = "New Orleans, USA",
    url = "https://arxiv.org/abs/2312.02406"
}

@phdthesis{phdthesis-pan-2022-QG,
  abbr = {PhD thesis},
  pdf = {https://www.proquest.com/docview/2723857718?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses},
  title = {Towards Generating Deep Questions from Text},
  author = {Liangming Pan},
  year = {2022},
  address = {Singapore},
  school = {National University of Singapore}
}